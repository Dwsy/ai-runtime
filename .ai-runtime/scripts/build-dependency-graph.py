#!/usr/bin/env python3
"""
AI Runtime - ä¾èµ–å…³ç³»å›¾è°±æ„å»ºå™¨
æ‰«æä»£ç åº“å¹¶æ„å»ºæ¨¡å—ä¾èµ–å…³ç³»å›¾ï¼Œè¯†åˆ«æ ¸å¿ƒèŠ‚ç‚¹å’Œç½‘ç»œæ‹“æ‰‘
"""

import os
import re
import json
import sys
from pathlib import Path
from collections import defaultdict
import networkx as nx

class DependencyGraphBuilder:
    def __init__(self, root_dir='.'):
        self.root_dir = Path(root_dir).resolve()
        self.graph = nx.DiGraph()
        self.files = []
        self.imports = defaultdict(list)
        self.imported_by = defaultdict(list)

    def scan_files(self):
        """æ‰«ææ‰€æœ‰ä»£ç æ–‡ä»¶"""
        exclude_dirs = {
            'node_modules', '.git', 'dist', 'build', 'coverage',
            '__pycache__', '.venv', '.ai-runtime'
        }

        file_patterns = ['*.js', '*.ts', '*.jsx', '*.tsx', '*.py', '*.java', '*.go']

        for pattern in file_patterns:
            for file_path in self.root_dir.rglob(pattern):
                # è·³è¿‡æ’é™¤ç›®å½•
                if any(exclude in str(file_path) for exclude in exclude_dirs):
                    continue

                self.files.append(file_path)

        print(f"ğŸ“‚ æ‰«æåˆ° {len(self.files)} ä¸ªä»£ç æ–‡ä»¶")

    def extract_imports(self, file_path):
        """ä»æ–‡ä»¶ä¸­æå–importè¯­å¥"""
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            imports = []
            rel_path = file_path.relative_to(self.root_dir)

            # JavaScript/TypeScript imports
            if file_path.suffix in ['.js', '.ts', '.jsx', '.tsx']:
                # import ... from './module'
                js_imports = re.findall(
                    r"(?:import|export)\s+.*?\s+from\s+['\"]([^'\"]+)['\"]",
                    content
                )
                # require('./module')
                js_imports += re.findall(
                    r"require\s*\(\s*['\"]([^'\"]+)['\"]\s*\)",
                    content
                )
                # import('./module')
                js_imports += re.findall(
                    r"import\s*\(\s*['\"]([^'\"]+)['\"]\s*\)",
                    content
                )

                for imp in js_imports:
                    # è§£æç›¸å¯¹è·¯å¾„
                    if imp.startswith('.'):
                        resolved = self.resolve_import_path(file_path, imp)
                        if resolved and resolved in self.files:
                            imports.append(resolved.relative_to(self.root_dir))

            # Python imports
            elif file_path.suffix == '.py':
                # import module
                py_imports = re.findall(r"^import\s+(\w+)", content, re.MULTILINE)
                # from module import ...
                py_imports += re.findall(r"^from\s+(\w+)\s+import", content, re.MULTILINE)

                for imp in py_imports:
                    # å°è¯•è§£æä¸ºæœ¬åœ°æ–‡ä»¶
                    possible_file = file_path.parent / f"{imp.replace('.', '/')}.py"
                    if possible_file.exists():
                        imports.append(possible_file.relative_to(self.root_dir))

            return list(set(imports))

        except Exception as e:
            print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []

    def resolve_import_path(self, current_file, import_path):
        """è§£æç›¸å¯¹å¯¼å…¥è·¯å¾„"""
        base = current_file.parent

        # å¤„ç† .js, .ts æ‰©å±•å
        extensions = ['', '.js', '.ts', '.jsx', '.tsx']

        for ext in extensions:
            if import_path.endswith('/'):
                # ç›®å½•å¯¼å…¥ï¼Œå°è¯• index.js
                test_path = base / import_path / 'index' / ext
            else:
                test_path = base / f"{import_path}{ext}"

            if test_path.exists():
                return test_path

        return None

    def build_graph(self):
        """æ„å»ºä¾èµ–å›¾"""
        print("ğŸ•¸ï¸  æ„å»ºä¾èµ–å…³ç³»å›¾...")

        # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
        for file_path in self.files:
            rel_path = file_path.relative_to(self.root_dir)
            self.graph.add_node(
                str(rel_path),
                type=self.get_file_type(file_path),
                size=file_path.stat().st_size,
                lines=self.count_lines(file_path)
            )

        # æå–å¹¶æ·»åŠ è¾¹
        for file_path in self.files:
            rel_path = file_path.relative_to(self.root_dir)
            imports = self.extract_imports(file_path)

            for imp in imports:
                self.imports[str(rel_path)].append(str(imp))
                self.imported_by[str(imp)].append(str(rel_path))

                # æ·»åŠ è¾¹
                self.graph.add_edge(
                    str(rel_path),
                    str(imp),
                    weight=1,
                    type='imports'
                )

                print(f"   {rel_path} â†’ {imp}")

        print(f"   å…±æ„å»º {self.graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹ï¼Œ{self.graph.number_of_edges()} æ¡è¾¹")

    def get_file_type(self, file_path):
        """è·å–æ–‡ä»¶ç±»å‹"""
        parts = str(file_path).split('/')
        if 'controllers' in parts:
            return 'controller'
        elif 'services' in parts:
            return 'service'
        elif 'repositories' in parts or 'models' in parts:
            return 'data'
        elif 'middleware' in parts:
            return 'middleware'
        elif 'utils' in parts or 'libs' in parts:
            return 'utility'
        elif 'test' in parts or 'spec' in parts:
            return 'test'
        else:
            return 'other'

    def count_lines(self, file_path):
        """ç»Ÿè®¡ä»£ç è¡Œæ•°"""
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            return len(content.splitlines())
        except:
            return 0

    def analyze_centrality(self):
        """åˆ†æèŠ‚ç‚¹ä¸­å¿ƒæ€§ï¼Œè¯†åˆ«æ ¸å¿ƒæ–‡ä»¶"""
        print("ğŸ”  åˆ†æç½‘ç»œä¸­å¿ƒæ€§...")

        try:
            # PageRankï¼ˆèŠ‚ç‚¹é‡è¦æ€§ï¼‰
            pagerank = nx.pagerank(self.graph, weight='weight')
            nx.set_node_attributes(self.graph, pagerank, 'pagerank')

            # ä»‹æ•°ä¸­å¿ƒæ€§ï¼ˆå…³é”®è·¯å¾„ï¼‰
            betweenness = nx.betweenness_centrality(self.graph)
            nx.set_node_attributes(self.graph, betweenness, 'betweenness')

            # åº¦æ•°ä¸­å¿ƒæ€§
            degree_centrality = nx.degree_centrality(self.graph)
            nx.set_node_attributes(self.graph, degree_centrality, 'degree')

            # è¯†åˆ«æ ¸å¿ƒèŠ‚ç‚¹
            core_nodes = [
                node for node, data in self.graph.nodes(data=True)
                if data.get('pagerank', 0) > 0.05
            ]

            print(f"   æ£€æµ‹åˆ° {len(core_nodes)} ä¸ªæ ¸å¿ƒèŠ‚ç‚¹")

        except Exception as e:
            print(f"âš ï¸  ä¸­å¿ƒæ€§åˆ†æå¤±è´¥: {e}")

    def detect_patterns(self):
        """æ£€æµ‹æ¶æ„æ¨¡å¼"""
        print("ğŸ§   è¯†åˆ«æ¶æ„æ¨¡å¼...")

        patterns = []
        nodes = [data for _, data in self.graph.nodes(data=True)]
        types = [data['type'] for data in nodes]

        # æ£€æµ‹åˆ†å±‚æ¶æ„
        if 'controller' in types and 'service' in types and 'data' in types:
            patterns.append({
                'name': 'Layered Architecture',
                'confidence': 0.85,
                'evidence': [
                    f"Controllers: {types.count('controller')} files",
                    f"Services: {types.count('service')} files",
                    f"Data layer: {types.count('data')} files"
                ]
            })

        # æ£€æµ‹MVC
        if 'controller' in types and 'data' in types and any('views' in str(path) for path in self.files):
            patterns.append({
                'name': 'MVC Pattern',
                'confidence': 0.75,
                'evidence': ['Controllers detected', 'Models detected', 'Views directory exists']
            })

        # æ£€æµ‹å¾®æœåŠ¡è¿¹è±¡
        package_jsons = list(self.root_dir.rglob('package.json'))
        if len(package_jsons) > 2:
            patterns.append({
                'name': 'Possible Microservices',
                'confidence': 0.6,
                'evidence': [f"{len(package_jsons)} package.json files found"]
            })

        print(f"   è¯†åˆ«åˆ° {len(patterns)} ä¸ªæ¶æ„æ¨¡å¼")

        return patterns

    def find_cycles(self):
        """æ£€æµ‹å¾ªç¯ä¾èµ–"""
        try:
            cycles = list(nx.simple_cycles(self.graph))
            if cycles:
                print(f"âš ï¸  å‘ç° {len(cycles)} ä¸ªå¾ªç¯ä¾èµ–")
                return cycles
            else:
                print("âœ… æœªå‘ç°å¾ªç¯ä¾èµ–")
                return []
        except:
            return []

    def generate_structured_data(self):
        """ç”Ÿæˆç»“æ„åŒ–è¾“å‡º"""
        print("ğŸ“Š ç”Ÿæˆç»“æ„åŒ–æ•°æ®...")

        data = {
            'metadata': {
                'scan_time': '2025-11-14',
                'file_count': len(self.files),
                'node_count': self.graph.number_of_nodes(),
                'edge_count': self.graph.number_of_edges()
            },
            'nodes': [
                {
                    'id': node,
                    **data
                }
                for node, data in self.graph.nodes(data=True)
            ],
            'edges': [
                {
                    'from': u,
                    'to': v,
                    **data
                }
                for u, v, data in self.graph.edges(data=True)
            ],
            'core_nodes': [
                {
                    'file': node,
                    'pagerank': data['pagerank'],
                    'betweenness': data['betweenness'],
                    'degree': data['degree']
                }
                for node, data in self.graph.nodes(data=True)
                if data.get('pagerank', 0) > 0.05
            ],
            'patterns': self.detect_patterns(),
            'cycles': self.find_cycles()
        }

        # æŒ‰PageRankæ’åºæ ¸å¿ƒèŠ‚ç‚¹
        data['core_nodes'].sort(key=lambda x: x['pagerank'], reverse=True)

        return data

    def save_graph(self, output_path='cognition/graphs/dependency-graph.json'):
        """ä¿å­˜ä¾èµ–å›¾"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        data = self.generate_structured_data()

        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)

        print(f"ğŸ’¾ ä¾èµ–å›¾å·²ä¿å­˜åˆ° {output_path}")

        # ä¿å­˜ä¸ºGraphMLæ ¼å¼ï¼ˆå¯ç”¨äºå¯è§†åŒ–å·¥å…·ï¼‰
        nx.write_graphml(self.graph, output_path.with_suffix('.graphml'))
        print(f"ğŸ’¾ GraphMLæ ¼å¼å·²ä¿å­˜åˆ° {output_path.with_suffix('.graphml')}")

        return data

def main():
    """ä¸»å…¥å£"""
    print("AI Runtime - ä¾èµ–å…³ç³»å›¾è°±æ„å»ºå™¨")
    print("=" * 40)

    builder = DependencyGraphBuilder()

    try:
        # æ‰«ææ–‡ä»¶
        builder.scan_files()

        # æ„å»ºå›¾è°±
        builder.build_graph()

        # åˆ†æä¸­å¿ƒæ€§
        builder.analyze_centrality()

        # ä¿å­˜ç»“æœ
        data = builder.save_graph()

        # æ‰“å°æ‘˜è¦
        print("\nğŸ“ˆ åˆ†ææŠ¥å‘Šæ‘˜è¦:")
        print(f"   æ ¸å¿ƒèŠ‚ç‚¹æ•°: {len(data['core_nodes'])}")
        print(f"   è¯†åˆ«æ¨¡å¼: {len(data['patterns'])}")
        print(f"   å¾ªç¯ä¾èµ–: {len(data['cycles'])}")

        if data['core_nodes']:
            print("\n   å‰3ä¸ªæ ¸å¿ƒæ–‡ä»¶:")
            for node in data['core_nodes'][:3]:
                print(f"     - {node['file']}: {node['pagerank']:.4f}")

        print("\nâœ… å®Œæˆï¼")

    except Exception as e:
        print(f"\
âŒ é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()
